<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title>Yangyang Guo</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr>
<!-- <td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About me</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Courses</a></div>
</td> -->
<td id="layout-content">
<div id="toptitle">
<h1>Yangyang Guo</h1>
</div>
<table class="imgtable">
<tr>
<td>
<img src="Avatar.jpeg" alt="alt text" width="121px" height="170px" />
</td>
<td align="left">
<p>Yangyang Guo (郭洋洋) <br />
Senior Research Fellow <br />
<a href="https://ncript.comp.nus.edu.sg/site/ncript/home/">School of Computing</a> <br />
<a href="https://nus.edu.sg/">National University of Singapore</a> <br />
Contact: <a href="mailto:guoyang.eric@gmail.com">Email</a>, <a href="https://scholar.google.com/citations?user=E4pqJ8wAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a>, <a href="https://github.com/guoyang9">Github</a></p>
</td>
</tr>
</table>

<h2>Short Bio</h2>
<p>I'm currently a senior research fellow with the National University of Singapore. <br />
I obtained my Doctor's Degree in the School of Computer Science and Technology, <br />
Shandong University, supervised by Prof. <a href="https://liqiangnie.github.io/index.html">Liqiang Nie</a>,
co-mentored by Prof. <a href="https://sites.google.com/view/zycheng">Zhiyong Cheng</a> from Hefei University of Technology. <br />
My research interests lie in the safety, bias, and efficiency problems of AI systems.
</p>

<h2>Education & Experiences</h2>
<p><b>National University of Singapore</b>, Oct. 2024 - Now</p>
<ul>
  <li>Senior Research Fellow, School of Computing</li>
  <li>Advisor: Prof. Mohan Kankanhalli</li>
</ul>
<p><b>National University of Singapore</b>, Aug. 2021 - Sep. 2024</p>
<ul>
  <li>Research Fellow, School of Computing</li>
  <li>Advisor: Prof. Mohan Kankanhalli</li>
</ul>
<p><b>Shandong University</b>, Sep. 2016 - Jun. 2021</p>
<ul>
  <li>PhD, Computer Science and Technology</li>
  <li>Advisor: Prof. Liqiang Nie; Co-Mentor: Prof. Zhiyong Cheng</li>
</ul>
<p><b>Alibaba Group</b>, Dec. 2020 - Apr. 2021</p>
<ul>
  <li>Research Intern</li>
</ul>
<p><b>National University of Singapore</b>, Oct. 2017 - Sep. 2018</p>
<ul>
  <li>Research Intern, School of Computing</li>
  <li>Advisor: Prof. Mohan Kankanhalli</li>
</ul>
<p><b>Ocean University of China</b>, Sep. 2012 - Jun. 2016</p>
<ul>
  <li>Undergraduate, Computer Science and Technology</li>
</ul>

<h2>Selected Publications (<a href="https://scholar.google.com/citations?user=E4pqJ8wAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a>)</h2>
<ul>
  <li><p>The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense.<br />
    <b>Yangyang Guo</b>, Fangkai Jiao, Liqiang Nie, Mohan Kankanhalli. <br />
    <i>NeurIPS, 2025. (Full Paper, acceptance rate: 24.5%) [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2411.08410">Paper</a>)</i></p>
  </li>
  <li><p>Fair Deepfake Detectors Can Generalize.<br />
    Harry Cheng, Ming-Hui Liu, <b>Yangyang Guo<sup>*</sup></b>, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli. <br />
    <i>NeurIPS, 2025. (Full Paper, acceptance rate: 24.5%) [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2507.02645">Paper</a>)</i></p>
  </li>
  <li><p>SCAN: Bootstrapping Contrastive Pre-training for Data Efficiency.<br />
    <b>Yangyang Guo</b>,  Mohan Kankanhalli. <br />
    <i>ICCV, 2025. (Full Paper, acceptance rate: 24%) [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2411.09126">Paper</a>  &amp;&amp; <a href="https://github.com/guoyang9/SCAN">Code</a>)</i></p>
  </li>
  <li><p>Social Debiasing for Fair Multi-modal LLMs.<br />
    Harry Cheng, <b>Yangyang Guo</b>, Qingpei Guo, Ming Yang, Tian Gan, Weili Guan, Liqiang Nie. <br />
    <i>ICCV, 2025. (Full Paper, acceptance rate: 24%) [<b>CCF A</b>]. (<a href="https://arxiv.org/abs/2408.06569">Paper</a>)</i></p>
  </li>
  <li><p>Joint Vision-Language Social Bias Removal for CLIP.<br />
    Haoyu Zhang, <b>Yangyang Guo<sup>*</sup></b>,  Mohan Kankanhalli. <br />
    <i>CVPR, 2025. (Full Paper, acceptance rate: 22.1%) [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2411.12785">Paper</a>  &amp;&amp; <a href="https://github.com/haoyusimon/VL_Debiasing">Code</a>)</i></p>
  </li>
  <li><p>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models.<br />
    <b>Yangyang Guo</b>, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli. <br />
    <i>IEEE TPAMI, 2024. [<b>CCF A</b>]. (<a href="https://arxiv.org/abs/2310.10942">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/UNK-VQA">Code</a>)</i></p>
  </li>
  <li><p>Diffusion Facial Forgery Detection.<br />
    Harry Cheng, <b>Yangyang Guo<sup>*</sup></b>,  Tianyi Wang, Liqiang Nie<sup>*</sup>, Mohan Kankanhalli. <br />
    <i>ACM MM, 2024. (Full Paper, acceptance rate: 26.2%) [<b>CCF A</b>]. (<a href="https://arxiv.org/abs/2401.15859">Paper</a>  &amp;&amp; <a href="https://github.com/xaCheng1996/DiFF">Code</a>)</i></p>
  </li>
  <li><p>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation.<br />
    <b>Yangyang Guo</b>, Guangzhi Wang, Mohan Kankanhalli. <br />
    <i>CVPR, 2024. (Full Paper, acceptance rate: 23.6%) [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2310.10700.pdf">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/PELA">Code</a>)</i></p>
  </li>
  <li><p>Bilateral Adaptation for Human-Object Interaction Detection with Occlusion-Robustness.<br />
    Guangzhi Wang, <b>Yangyang Guo</b>, Ziwei Xu, Mohan Kankanhalli. <br />
    <i>CVPR, 2024. (Full Paper, acceptance rate: 23.6%) [<b>CCF A</b>]. (<a href="">Paper</a> &amp;&amp; <a href="">Code</a>)</i></p>
  </li>
  <li><p>Voice-face Homogeneity Tells Deepfake.<br />
    Harry Cheng, <b>Yangyang Guo<sup></b>,  Tianyi Wang, Qi Li, Xiaojun Chang, Liqiang Nie<sup>*</sup>. <br />
    <i>ACM ToMM, 2024. [<b>CCF B</b>]. (<a href="https://arxiv.org/pdf/2203.02195.pdf">Paper</a> &amp;&amp; <a href="https://github.com/xaCheng1996/MVF">Code</a>)</i></p>
  </li>
  <li><p>Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration.<br />
    Harry Cheng, <b>Yangyang Guo<sup>*</sup></b>, Liqiang Nie<sup>*</sup>, Zhiyong Cheng, Mohan Kankanhalli. <br />
    <i>ACM MM, 2023. (Full Paper, acceptance rate: 29.3%). [<b>CCF A</b>]. (<a href="https://arxiv.org/pdf/2307.14866.pdf">Paper</a> &amp;&amp; <a href="https://github.com/xaCheng1996/SLLM">Code</a>)</i></p>
  </li>
  <li><p>Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR.<br />
    Zhenyang Li, <b>Yangyang Guo</b>, Kejie Wang, Xiaolin Chen, Liqiang Nie, Mohan Kankanhalli. <br />
    <i>ACM MM, 2023. (Full Paper, acceptance rate: 29.3%). [<b>CCF A</b>]. (<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612395">Paper</a>)</i></p>
  </li>
  <li><p>On Modality Bias Recognition and Reduction.<br />
    <b>Yangyang Guo</b>, Liqiang Nie, Harry Cheng, Zhiyong Cheng, Mohan Kankanhalli, Alberto Del Bimbo. <br />
    <i>ACM ToMM, 2023. [<b>CCF B</b>] (<a href="https://arxiv.org/pdf/2202.12690.pdf">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/AdaVQA">Code</a>)</i></p>
  </li>
  <li><p>Joint Answering and Explanation for Visual Commonsense Reasoning.<br />
    Zhenyang Li, <b>Yangyang Guo<sup>*</sup></b>, Kejie Wang, Yinwei Wei, Liqiang Nie<sup>*</sup>, Mohan Kankanhalli. <br />
    <i>IEEE TIP, 2023. [<b>CCF A</b>] (<a href="https://arxiv.org/pdf/2202.12626.pdf">Paper</a> &amp;&amp; <a href="https://github.com/SDLZY/ARC">Code</a>)</i></p>
  </li>
  <li><p>Chairs Can be Stood on: Overcoming Object Bias in Human-Object Interaction Detection.<br />
    Guangzhi Wang, <b>Yangyang Guo<sup>*</sup></b>, Yongkang Wong, Mohan Kankanhalli. <br />
    <i>ECCV, 2022. (Full Paper, acceptance rate: 28.4%). [<b>CCF B</b>] (<a href="https://arxiv.org/abs/2207.02400">Paper</a> &amp;&amp; <a href="https://github.com/daoyuan98/ODM">Code</a>)</i></p>
  </li>
  <li><p>Compute to Tell the Tale: Goal-Driven Narrative Generation.<br />
    Yongkang Wong, Shaojing Fan, <b>Yangyang Guo</b>, Ziwei Xu, Karen Stephen, Rishabh Sheoran, Anusha Bhamidipati, Vivek Barsopia, Jianquan Liu, Mohan Kankanhalli. <br />
    <i>ACM MM BNI (<b><font color="#E54841">Best BNI Award</font></b>), 2022, [<b>CCF A</b>]. (<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3549202">Paper</a>)</i></p>
  </li>
  <li><p>A Unified End-to-End Retriever-Reader Framework for Knowledge-based VQA.<br />
    <b>Yangyang Guo</b>, Liqiang Nie, Yongkang Wong, Yibing Liu, Zhiyong Cheng, Mohan Kankanhalli. <br />
    <i>ACM MM, 2022. (Full Paper, acceptance rate: 27.9%). [<b>CCF A</b>] (<a href="https://arxiv.org/abs/2206.14989">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/UnifER">Code</a>)</i></p>
  </li>
  <li><p>Distance Matters in Human-Object Interaction Detection.<br />
    Guangzhi Wang, <b>Yangyang Guo<sup>*</sup></b>, Yongkang Wong, Mohan Kankanhalli. <br />
    <i>ACM MM, 2022. (Full Paper, acceptance rate: 27.9%). [<b>CCF A</b>] (<a href="https://arxiv.org/abs/2207.01869">Paper</a> &amp;&amp; <a href="https://github.com/daoyuan98/SDT-HOI">Code</a>)</i></p>
  </li>
  <li><p>Rethinking Attention-Model Explainability through Faithfulness Violation Test.<br />
    Yibing Liu, Haoliang Li, <b>Yangyang Guo</b>, Chenqi Kong, Jing Li, Shiqi Wang. <br />
    <i>ICML, 2022. (Full Paper, acceptance rate: 21.9%). [<b>CCF A (Spotlight)</b>] (<a href="https://arxiv.org/pdf/2201.12114.pdf">Paper</a> &amp;&amp; <a href="https://github.com/BierOne/Attention-Faithfulness">Code</a>)</i></p>
  </li>
  <li><p>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning.<br />
    Fangkai Jiao, <b>Yangyang Guo<sup>*</sup></b>, Xuemeng Song, Liqiang Nie<sup>*</sup>. <br />
    <i>ACL, 2022. (Findings, Full Paper). [<b>CCF A</b>] (<a href="https://arxiv.org/pdf/2203.00357.pdf">Paper</a> &amp;&amp; <a href="https://github.com/SparkJiao/MERIt">Code</a>)</i></p>
  </li>
  <li><p>Answer Questions with Right Image Regions: A Visual Attention Regularization Approach.<br />
    Yibing Liu, <b>Yangyang Guo</b>, Jianhua Yin, Xuemeng Song, Weifeng Liu, Liqiang Nie, Min Zhang. <br />
    <i>ACM ToMM, 2022. [<b>CCF B</b>] (<a href="https://arxiv.org/pdf/2102.01916.pdf">Paper</a> &amp;&amp; <a href="https://github.com/BierOne/VQA-AttReg">Code</a>)</i></p>
  </li>
  <li><p>Loss Re-scaling VQA: Revisiting the Language Prior Problem from a Class-imbalance View.<br />
    <b>Yangyang Guo</b>, Liqiang Nie, Zhiyong Cheng, Qi Tian, Min Zhang. <br />
    <i>IEEE TIP, 2021. [<b>CCF A</b>] (<a href="https://arxiv.org/pdf/2010.16010.pdf">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/class-imbalance-VQA">Code</a>)</i></p>
  </li>
  <li><p>AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss.<br />
    <b>Yangyang Guo</b>, Liqiang Nie, Zhiyong Cheng, Feng Ji, Ji Zhang, Alberto Del Bimbo. <br />
    <i>IJCAI, 2021. (Full Paper, acceptance rate: 13.9%). [<b>CCF A</b>] (<a href="https://www.ijcai.org/proceedings/2021/0098.pdf">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/AdaVQA">Code</a>)</i></p>
  </li>
  <li><p>Quantifying and Alleviating the Language Prior Problem in Visual Question Answering.<br />
    <b>Yangyang Guo</b>, Zhiyong Cheng, Liqiang Nie, Yibing Liu, Yinglong Wang and Mohan Kankanhalli. <br />
    <i>ACM SIGIR, 2019. (Full Paper, acceptance rate: 20%). [<b>CCF A</b>] (<a href="https://dl.acm.org/doi/10.1145/3331184.3331186">Paper</a> &amp;&amp; <a href="https://github.com/guoyang9/vqa-prior">Code</a>)</i></p>
  </li>
</ul>

<h2>Academic Services</h2>
<p><b>Area Chair</b></p>
<ul>
  <li> IJCAI 2025 (Meta-Reviewer)</li>
  <li> ACM MM 2025 (Area Chair)</li>
</ul>
<p><b>Conference Reviewer</b></p>
<ul> 
  <li>CVPR 2022-2025, ICCV 2023-2025, ECCV 2022-2024</li>
  <li>ACM MM 2020-2024, MM Asia 2020-2022</li>
  <li>AAAI 2021-2026, IJCAI 2023-2024, ICML 2023-2025, NeurIPS 2023-2025, ICLR 2024-2026</li>
  <li>KDD 2022, WSDM 2022-2023</li>
</ul>
<p><b>Journal Reviewer</b></p>
<ul>
  <li>IEEE TPAMI, TIP, TIFS, TKDE, TMM, TCSVT</li>
  <li>ACM ToMM, TOIS, TWEB</li>
</ul>

<h2>Projects</h2>
<p><b>Chinese language LLM safety Evaluator</b></p>
<ul>
  <li> <b><font color="#E54841">Team Leader</font></b> </li>
  <li> Team Members: Yangyang Guo, Ziwei Xu, Jiahao Lu, Yongkang Wong, and Mohan Kankanhalli.</li>
  <li> Collaborated with ML Commons and IMDA Singapore. </li>
</ul>
  
<h2>Honors</h2>
<ul>
  <li>Champion of the ICML 2024 TiFA Workshop MLLM Attack Challenge</li>
  <ul>
    <li> <b><font color="#E54841">Team Leader</font></b> </li>
    <li> Team Members: Yangyang Guo, Ziwei Xu, Xilie Xu, Yongkang Wong, Liqiang Nie, and Mohan Kankanhalli.</li>
  </ul>
  </li>
  <li>Outstanding PC Award for WSDM, 2022</li>
  <li>Outstanding Reviewer Award for IEEE TMM, 2021</li>
  <li>Shandong University President Scholarship, 2019</li>
  <li>National Scholarship for Ph.D. students, 2019</li>
  <li>ACM SIGIR Student Travel Grant, 2019</li>
  <li>ACM MM Student Travel Grant, 2018</li>
</ul>
</td>
</tr>
</table>
</body>
</html>

